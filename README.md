# attentions_pytorch
A repository for implementations of attention mechanism by PyTorch.

## Introduction
This repository contains implementations of attention mechanism by PyTorch.

## Implementations
- [Dot-Product Attention](https://arxiv.org/abs/1508.04025)
- [Scaled Dot-Product Attention](https://arxiv.org/abs/1706.03762?amp=1)
- [General Attention](https://arxiv.org/abs/1508.04025)
- [Multi Head Attention](https://arxiv.org/abs/1706.03762?amp=1)

## Author
- SeungHyun Lee [@whsqkaak](https://github.com/whsqkaak)
- Contacts: whsqkaak@naver.com
